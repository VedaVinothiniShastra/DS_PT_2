{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25901e3e",
   "metadata": {},
   "source": [
    "Binary Classifiers\t\n",
    "    Evaluating Binary Classifier Predictions -> f1, recall , precision , condusion matrix ,  roc curve,log loss\n",
    "\tEvaluating Binary Classifier Thresholds\n",
    "\tEvaluating Multiclass Classifier Predictions\n",
    "        1.all input data is not balanced, hence the issue of Imbalanced Classes\n",
    "        2.With the Accuracy Evaluation Metric removed from our options, we specifically turn to Precision, Recall, and F1 Scores \n",
    "        3.We use parameter options in Python, which are used for aggregating the evaluation values by averaging them. \n",
    "        4.The three main options that we have available to us are: \n",
    "            a._macro – Here we specify to the compiler to calculate the mean of metric scores for each class in the dataset, weighting each class equally. \n",
    "            b._weighted – We calculate the mean of metric scores for each class, and we weigh each class directly proportional to its size in the dataset. \n",
    "            c._micro – Here we calculate the mean of metric scores for each OBSERVATION in the dataset.\n",
    "\n",
    "Regression Models\t\n",
    "    Visualizing a Classifier’s Performance\n",
    "\tEvaluating Regression Models\n",
    "\tEvaluating Clustering Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "925e4bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.956140350877193\n",
      "[[39  4]\n",
      " [ 1 70]]\n"
     ]
    }
   ],
   "source": [
    "#visualizing classifier performance\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "cancer = datasets.load_breast_cancer()\n",
    "X = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "y = cancer.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "classifier = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
    "print(classifier.score(X_test, y_test))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587da2f8",
   "metadata": {},
   "source": [
    "TP, TN, FP, FN \n",
    "->TP represents the number of True Positives. This refers to the total number of observations that belong to the positive class and have been predicted correctly. \n",
    "->TN represents the number of True Negatives. This is the total number of observations that belong to the negative class and have been predicted correctly. \n",
    "->FP is the number of False Positives\n",
    "->FN is the number of False Negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fbf813",
   "metadata": {},
   "source": [
    "#Evaluating Regression Models:\n",
    "\n",
    "1.Mean Absolute Error(MAE) ->MAE is a very simple metric which calculates the absolute difference between actual and predicted values \n",
    "    Advantages of MAE \n",
    "        1.The MAE you get is in the same unit as the output variable. \n",
    "        2.It is most Robust to outliers. \n",
    "    Disadvantages of MAE \n",
    "        1.The graph of MAE is not differentiable so we have to apply various optimizers like Gradient descent which can be differentiable.\n",
    "\n",
    "2.Mean Squared Error(MSE) ->Mean squared error states that finding the squared difference between actual and predicted value \n",
    "    Advantages of MSE \n",
    "        1.The graph of MSE is differentiable, so you can easily use it as a loss function. \n",
    "    Disadvantages of MSE \n",
    "        1.The value you get after calculating MSE is a squared unit of output. for example, the output variable is in meter(m) then after calculating MSE the output we get is in meter squared. \n",
    "        2.If you have outliers in the dataset then it penalizes the outliers most and the calculated MSE is bigger. So, in short, It is not Robust to outliers which were an advantage in MAE.\n",
    "\n",
    "3.RMSE ->simple square root of mean squared error Advantages of RMSE 1.The output value you get is in the same unit as the required output variable which makes interpretation of loss easy. Disadvantages of RMSE 1.It is not that robust to outliers as compared to MAE. for performing RMSE we have to NumPy NumPy square root function over MSE. \n",
    "\n",
    "4.RMSLE- Root Mean Squared Log Error -> Used in all ML proj log of calculated RMSE error and resultant we get as RMSLE\n",
    "\n",
    "5.R squared R2 squared is also known as Coefficient of Determination or sometimes also known as Goodness of fit. R^2=1- SSR/SSM (SSR->Squared sum error of regression line, SSM->Squared sum error of mean line) 0-> less 0.8-.normal 1\n",
    "\n",
    "->Perfect Cons: If u add new features it will start from first \n",
    "\n",
    "6.Adjusted R Squares R^2a = 1 - ((1-r2)*(n-1)/(n-k-1)) n-> no of observation k=no of independent variables R^2a = Adjuested R^2a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd10753",
   "metadata": {},
   "source": [
    "MAE:\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "print(\"MAE\",mean_absolute_error(y_test,y_pred))\n",
    "\n",
    "MSE\n",
    "from sklearn.metrics import mean_squared_error \n",
    "print(\"MSE\",mean_squared_error(y_test,y_pred))\n",
    "\n",
    "RMSE\n",
    "print(\"RMSE\",np.sqrt(mean_squared_error(y_test,y_pred)))\n",
    "\n",
    "RMSLE\n",
    "print(\"RMSLE\",np.log(np.sqrt(mean_squared_error(y_test,y_pred))))\n",
    "\n",
    "R Sqaured\n",
    "from sklearn.metrics import r2_score \n",
    "r2 = r2_score(y_test,y_pred) print(r2)\n",
    "\n",
    "Adjusted R^2\n",
    "n=40 k=2 \n",
    "adj_r2_score = 1 - ((1-r2)*(n-1)/(n-k-1)) \n",
    "print(adj_r2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab0b4187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE 0.043859649122807015\n",
      "MSE 0.043859649122807015\n",
      "RMSE 0.20942695414584775\n",
      "RMSLE -1.5633802679801976\n",
      "R Squared 0.8132983950212905\n"
     ]
    }
   ],
   "source": [
    "#Evaluating Regression Models\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "cancer = datasets.load_breast_cancer()\n",
    "X = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "y = cancer.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "clf = LogisticRegression(max_iter=10000, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"MAE\",mean_absolute_error(y_test,y_pred))\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"MSE\",mean_squared_error(y_test,y_pred))\n",
    "print(\"RMSE\",np.sqrt(mean_squared_error(y_test,y_pred)))\n",
    "print(\"RMSLE\",np.log(np.sqrt(mean_squared_error(y_test,y_pred))))\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_test,y_pred)\n",
    "print(\"R Squared\",r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95da148",
   "metadata": {},
   "source": [
    "Evaluation metrix for clustering\n",
    "\n",
    "1.Adjusted Rand Index ARI = (RI - Expected_RI) / (max(RI) - Expected_RI)\n",
    "\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "ari = adjusted_rand_score(target, model.labels_) print(ari)\n",
    "\n",
    "2.Rand Index RI = (number of agreeing pairs) / (number of pairs)\n",
    "\n",
    "from sklearn.metrics import rand_score\n",
    "ris = rand_score(target, model.labels_) \n",
    "print(ris)\n",
    "\n",
    "3.Silhouette Score aka Silhouette Coefficient -1 ->poor 1-> good 0-> Overlappinf of clusters\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "ss = silhouette_score(feature, model.labels_) \n",
    "print(ss)\n",
    "\n",
    "4.Davies-Bouldin Index ratio of within-cluster distances to between-cluster distances 0-> poor 1->good\n",
    "\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "dbs = davies_bouldin_score(feature, model.labels_) \n",
    "print(dbs)\n",
    "\n",
    "5.Mutual Information\n",
    "\n",
    "Mutual Information between two clusters is a measure of the similarity between two labels of the same data\n",
    "\n",
    "from sklearn.metrics import mutual_info_score \n",
    "mis = mutual_info_score(target, model.labels_) print(mis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7a6e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bjawa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI 0.7812362998684788\n",
      "RI 0.9198396793587175\n",
      "Silhouette 0.7328381899726921\n"
     ]
    }
   ],
   "source": [
    "#Evaluating Clustering Models\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feature, target = make_blobs(n_samples=500,centers=5,random_state=42,shuffle=False)\n",
    "plt.scatter(feature[:, 0], feature[:, 1])\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "model = KMeans(n_clusters=4)\n",
    "model.fit(feature)\n",
    "plt.scatter(feature[:, 0], feature[:, 1], color=\"r\")\n",
    "plt.scatter(model.cluster_centers_[1],model.cluster_centers_[3],color=\"k\", marker=\"*\")\n",
    "plt.scatter(model.cluster_centers_[2],model.cluster_centers_[0],color=\"k\", marker=\"*\")\n",
    "\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "ari = adjusted_rand_score(target, model.labels_)\n",
    "print(\"ARI\",ari)\n",
    "from sklearn.metrics import rand_score\n",
    "\n",
    "ris = rand_score(target, model.labels_)\n",
    "print(\"RI\",ris)\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "ss = silhouette_score(feature, model.labels_)\n",
    "print(\"Silhouette\",ss)\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "dbs = davies_bouldin_score(feature, model.labels_)\n",
    "print(\"Davies Bouldin\",dbs)\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "mis = mutual_info_score(target, model.labels_)\n",
    "print(\"Mutual Information \",mis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7e0b76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
