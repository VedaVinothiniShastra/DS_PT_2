{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fc45660",
   "metadata": {},
   "source": [
    "Speeding the model execution\t\n",
    "    Selecting Best Models When Preprocessing\n",
    "\n",
    "    1.Rescale Data -> MinMaxScaler (Sklearn)\n",
    "    2.Binarize Data (Make Binary)\n",
    "    3.Standardize Data\n",
    "    \n",
    "\tSpeeding Up Model Selection with Parallelization\n",
    "\tSpeeding Up Model Selection Using Algorithm-Specific Methods\n",
    "\tEvaluating Performance After Model Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f292cdce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     preg  plas  pres  skin  test  mass   pedi  age  class\n",
      "0       6   148    72    35     0  33.6  0.627   50      1\n",
      "1       1    85    66    29     0  26.6  0.351   31      0\n",
      "2       8   183    64     0     0  23.3  0.672   32      1\n",
      "3       1    89    66    23    94  28.1  0.167   21      0\n",
      "4       0   137    40    35   168  43.1  2.288   33      1\n",
      "..    ...   ...   ...   ...   ...   ...    ...  ...    ...\n",
      "763    10   101    76    48   180  32.9  0.171   63      0\n",
      "764     2   122    70    27     0  36.8  0.340   27      0\n",
      "765     5   121    72    23   112  26.2  0.245   30      0\n",
      "766     1   126    60     0     0  30.1  0.349   47      1\n",
      "767     1    93    70    31     0  30.4  0.315   23      0\n",
      "\n",
      "[768 rows x 9 columns]\n",
      "[[0.353 0.744 0.59  0.354 0.    0.501 0.234 0.483]\n",
      " [0.059 0.427 0.541 0.293 0.    0.396 0.117 0.167]\n",
      " [0.471 0.92  0.525 0.    0.    0.347 0.254 0.183]\n",
      " [0.059 0.447 0.541 0.232 0.111 0.419 0.038 0.   ]\n",
      " [0.    0.688 0.328 0.354 0.199 0.642 0.944 0.2  ]]\n"
     ]
    }
   ],
   "source": [
    "#minmaxscaler\n",
    "import pandas\n",
    "import scipy\n",
    "import numpy\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "url=\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names = names)\n",
    "array = dataframe.values\n",
    "print(dataframe)\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "scaler=MinMaxScaler(feature_range=(0, 1))\n",
    "rescaledX = scaler.fit_transform(X)\n",
    "numpy.set_printoptions(precision=3)  \n",
    "print(rescaledX[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cabfcd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     preg  plas  pres  skin  test  mass   pedi  age  class\n",
      "0       6   148    72    35     0  33.6  0.627   50      1\n",
      "1       1    85    66    29     0  26.6  0.351   31      0\n",
      "2       8   183    64     0     0  23.3  0.672   32      1\n",
      "3       1    89    66    23    94  28.1  0.167   21      0\n",
      "4       0   137    40    35   168  43.1  2.288   33      1\n",
      "..    ...   ...   ...   ...   ...   ...    ...  ...    ...\n",
      "763    10   101    76    48   180  32.9  0.171   63      0\n",
      "764     2   122    70    27     0  36.8  0.340   27      0\n",
      "765     5   121    72    23   112  26.2  0.245   30      0\n",
      "766     1   126    60     0     0  30.1  0.349   47      1\n",
      "767     1    93    70    31     0  30.4  0.315   23      0\n",
      "\n",
      "[768 rows x 9 columns]\n",
      "[[1. 1. 1. 1. 0. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 0. 1. 1. 1.]\n",
      " [1. 1. 1. 0. 0. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 1. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#Binarizer\n",
    "import pandas\n",
    "import scipy\n",
    "import numpy\n",
    "from sklearn.preprocessing import Binarizer\n",
    "url=\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names = names)\n",
    "array = dataframe.values\n",
    "print(dataframe)\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "binarizer = Binarizer(threshold = 0.0).fit(X)\n",
    "binaryX = binarizer.transform(X)\n",
    "numpy.set_printoptions(precision = 3)\n",
    "print(binaryX[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8121856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     preg  plas  pres  skin  test  mass   pedi  age  class\n",
      "0       6   148    72    35     0  33.6  0.627   50      1\n",
      "1       1    85    66    29     0  26.6  0.351   31      0\n",
      "2       8   183    64     0     0  23.3  0.672   32      1\n",
      "3       1    89    66    23    94  28.1  0.167   21      0\n",
      "4       0   137    40    35   168  43.1  2.288   33      1\n",
      "..    ...   ...   ...   ...   ...   ...    ...  ...    ...\n",
      "763    10   101    76    48   180  32.9  0.171   63      0\n",
      "764     2   122    70    27     0  36.8  0.340   27      0\n",
      "765     5   121    72    23   112  26.2  0.245   30      0\n",
      "766     1   126    60     0     0  30.1  0.349   47      1\n",
      "767     1    93    70    31     0  30.4  0.315   23      0\n",
      "\n",
      "[768 rows x 9 columns]\n",
      "[[ 0.64   0.848  0.15   0.907 -0.693  0.204  0.468  1.426]\n",
      " [-0.845 -1.123 -0.161  0.531 -0.693 -0.684 -0.365 -0.191]\n",
      " [ 1.234  1.944 -0.264 -1.288 -0.693 -1.103  0.604 -0.106]\n",
      " [-0.845 -0.998 -0.161  0.155  0.123 -0.494 -0.921 -1.042]\n",
      " [-1.142  0.504 -1.505  0.907  0.766  1.41   5.485 -0.02 ]]\n"
     ]
    }
   ],
   "source": [
    "#StandardScaler\n",
    "\n",
    "import pandas\n",
    "import scipy\n",
    "import numpy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "url=\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names = names)\n",
    "array = dataframe.values\n",
    "print(dataframe)\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "scaler = StandardScaler().fit(X)\n",
    "rescaledX = scaler.transform(X)\n",
    "numpy.set_printoptions(precision = 3)\n",
    "print(rescaledX[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eac071c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Speeding Up Model Selection with Parallelization\n",
    "refer : https://github.com/VedaVinothiniShastra/DS_PT_2/blob/master/Speeding_Up_Model_Selection_with_Parallelization.ipynb\n",
    "        \n",
    "1.Using Pandas directly with two threads \n",
    "2.Using Dask with threads and separate processes \n",
    "3.Using Modin with a Ray/ Dask backend \n",
    "4.Using multiprocessing.Pool to launch separate processes \n",
    "5.Using joblib.parallel to launch separate threads and processes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c567d08a",
   "metadata": {},
   "source": [
    "#Speeding Up Model Selection Using Algorithm-Specific Methods\n",
    "\n",
    "\n",
    "1.Probabilistic Measures -> Choose a model via in-sample error and complexity\n",
    "\n",
    "a.Akaike Information Criterion (AIC)->method for scoring and selecting a model\n",
    "    AIC = -2/N * LL + 2 * k/N\n",
    "    LL-> is the log-likelihood of the model\n",
    "    N ->is the number of examples in the training dataset\n",
    "    k ->is the number of parameters in the model\n",
    "\n",
    "b.Bayesian Information Criterion (BIC)->the model with the lowest BIC is selected\n",
    "    BIC = -2 * LL + log(N) * k\n",
    "    LL-> is the log-likelihood of the model\n",
    "    N ->is the number of examples in the training dataset\n",
    "    k ->is the number of parameters in the model\n",
    "\n",
    "c.Minimum Description Length (MDL)->the model with the lowest MDL is selected\n",
    "    MDL = L(h) + L(D | h)\n",
    "    h -> is the model\n",
    "    D -> is the predictions made by the model\n",
    "    L(h)-> is the number of bits required to represent the model\n",
    "    L(D | h)-> is the number of bits required to represent the predictions from the model on the training dataset.\n",
    "\n",
    "\n",
    "d.Structural Risk Minimization (SRM)->balancing the model's complexity against its success at fitting the training data\n",
    "\n",
    "2.Resampling Methods ->Choose a model via estimated out-of-sample error \n",
    "    a.Random train/test splits. \n",
    "    b.Cross-Validation (k-fold, LOOCV, etc.). \n",
    "    c.Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "219b01bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 3\n",
      "0.01003905094353046\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "X, y = make_regression(n_samples=100, n_features=2, noise=0.1)\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "num_params = len(model.coef_) + 1\n",
    "print('Number of parameters: %d' % (num_params))\n",
    "yhat = model.predict(X)\n",
    "mse = mean_squared_error(y, yhat)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "659c727d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIC -454.12726967233704\n",
      "BIC -446.31175911437276\n"
     ]
    }
   ],
   "source": [
    "def calculate_aic(n, mse, num_params):\n",
    " aic = n * log(mse) + 2 * num_params\n",
    " return aic\n",
    "def calculate_bic(n, mse, num_params):\n",
    " bic = n * log(mse) + num_params * log(n)\n",
    " return bic\n",
    "\n",
    "from math import log\n",
    "aic = calculate_aic(len(y), mse, num_params)\n",
    "print('AIC',aic)\n",
    "bic = calculate_bic(len(y), mse, num_params)\n",
    "print('BIC',bic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7186a4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99\n"
     ]
    }
   ],
   "source": [
    "#LOOCV\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = make_blobs(n_samples=100, random_state=1)\n",
    "cv = LeaveOneOut()\n",
    "y_true, y_pred = list(), list()\n",
    "for train_ix, test_ix in cv.split(X):\n",
    "    X_train, X_test = X[train_ix, :], X[test_ix, :]\n",
    "    y_train, y_test = y[train_ix], y[test_ix]\n",
    "    model = RandomForestClassifier(random_state=1)\n",
    "    model.fit(X_train, y_train)\n",
    "    yhat = model.predict(X_test)\n",
    "    y_true.append(y_test[0])\n",
    "    y_pred.append(yhat[0])\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5657e41c",
   "metadata": {},
   "source": [
    "Neural Networks\tIntroduction - Capstone 1\n",
    "\tPreprocessing Data for Neural Networks\n",
    "\tDesigning a Neural Network\n",
    "\tTraining a Binary Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b2458b",
   "metadata": {},
   "source": [
    "Neural Networks ->Inspired by biological neural networks Introduction Supervised, Unsupervised Types of NN: \n",
    "1.multilayer perceptron ->uses a nonlinear activation function. \n",
    "2.convolutional neural network CNN ->used multilayer perceptrons. \n",
    "3.recursive neural network RNN ->uses weights to make structured predictions \n",
    "4.recurrent neural network RNN ->use recurrent neural network architecture and does not use an activation function \n",
    "5.sequence-to-sequence modules (2 Recuurent NN & Shallow Nn)->use two recurrent networks and shallow neural networks which produce a vector space from an amount of text\n",
    "\n",
    "Preprocess:\n",
    "1.Min-Max Scaling\n",
    "2.Decimal Scaling\n",
    "3.Eliminating Outliers\n",
    "4.Z-score normalization or Standardization\n",
    "5.Mean / Median Absolute Deviation\n",
    "6.(Modified) Tanh Estimator\n",
    "7.Max-Scaling\n",
    "\n",
    "Libraries :\n",
    "Tensorlfow\n",
    "Pytorch\n",
    "Keras\n",
    "Opencv (Computer Vision)\n",
    "\n",
    "26,27,31, 32 - One project - Capstone 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc7a323",
   "metadata": {},
   "source": [
    "Neural Networks Multiclass Classifier\t - Fruit classification\n",
    "    Training a Multiclass Classifier\n",
    "\tTraining a Regressor\n",
    "\tMaking Predictions\n",
    "Addressing Overfitting Problem\t\n",
    "    Visualize Training History\n",
    "\tReducing Overfitting with Weight Regularization\n",
    "\tReducing Overfitting with Early Stopping\n",
    "\tReducing Overfitting with Dropout\n",
    "Neural Networks - Cross Fold Validation\t\n",
    "    Saving Model Training Progress\n",
    "\tk-Fold Cross-Validating Neural Networks\n",
    "\tTuning Neural Networks\n",
    "\tVisualizing Neural Networks\n",
    "Neural Networks - Classification\t\n",
    "    Classifying Images - Cap 1 \n",
    "\tImproving Performance with Image Augmentation\n",
    "\tClassifying Text\n",
    "\tSaving and Loading a scikit-learn Model\n",
    "    \n",
    "    \n",
    "Keras\t\n",
    "    Introduction\n",
    "\tTensorflow\n",
    "\tTheano and CNTK\n",
    "\tDeveloping with Keras\n",
    "\tModels\n",
    "\tNetwork of Layers\n",
    "Setting up Keras\tLoss Functions\n",
    "\tRunning deep learning on the cloud\n",
    "\tPre-processing techniques\n",
    "Deep learning for computer vision\t\n",
    "    Convolutional Neural Networks\n",
    "\tMax-pooling\n",
    "\tDownloading the data\n",
    "\tBuilding the network\n",
    "\tData Pre-processing\n",
    "\tData Augmentation\n",
    "Feature Extraction with Keras\t\n",
    "    Feature Extraction\n",
    "\tFine-tuning\n",
    "\tTraining-Testing Split\n",
    "Visualizing\t\n",
    "    Visualizing immediate activations\n",
    "\tVisualizing convnet filters\n",
    "\tVisualizing heatmaps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
