{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c1927d1",
   "metadata": {},
   "source": [
    "Hyperparameters and Model Validation\t\n",
    "    Thinking About Model Validation -> Telling the model is good or bad\n",
    "\tSelecting the Best Model -> Classifcaition -> Naive , decision , random forest,SVM -SVC (Feasible slns) --> (Optimal/ Best Sln)\n",
    "    \n",
    "\tLearning Curves  -> DL , NN\n",
    "\tValidation in Practice: Grid Search\n",
    "Feature Engineering\t\n",
    "    Categorical Features\n",
    "\tText Features\n",
    "\tImage Features\n",
    "\tDerived Features\n",
    "\tImputation of Missing Data\n",
    "\tFeature Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d06c03",
   "metadata": {},
   "source": [
    "Classification:\n",
    "    1.F1 score: What is F1 score? \n",
    "        F1 score (also known as F-measure, or balanced F-score) is an error metric which measures model performance by calculating the harmonic mean of precision and recall for the minority positive class.\n",
    "        F1 score = 2 * (Precision * recall)/(precision +recall)\n",
    "   eg: \n",
    "   from sklearn.metrics import f1_score\n",
    "    y_true = [0, 1, 0, 0, 1, 1]\n",
    "    y_pred = [0, 0, 1, 0, 0, 1]\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    \n",
    "    > 0.9 Very good\n",
    "    0.8 - 0.9 Good \n",
    "    0.5 - 0.8 OK \n",
    "    < 0.5 Not good\n",
    "    \n",
    "   1.A model will obtain a high F1 score if both Precision and Recall are high \n",
    "   2.A model will obtain a low F1 score if both Precision and Recall are low \n",
    "   3.A model will obtain a medium F1 score if one of Precision and Recall is low and the other is high\n",
    "   \n",
    "   \n",
    " 2.Confusion matrix ->display the performance of model\n",
    " \n",
    "  True , False \n",
    "  Positive or Negative\n",
    "  \n",
    " \n",
    "  1.True Positive: This combination tells us how many times a model correctly classifies a positive sample as Positive? \n",
    "   eg: Gender classifier model -> Female image => yes it is female\n",
    "   \n",
    "  2.False Negative: This combination tells us how many times a model incorrectly classifies a positive sample as Negative?\n",
    "   eg: Gender classifier model -> Female image => It is a Male \n",
    "   \n",
    "  3.False Positive: This combination tells us how many times a model incorrectly classifies a negative sample as Positive?\n",
    "    eg: Gender classifier model -> Female image => It is not a male  \n",
    "  \n",
    "  4.True Negative: This combination tells us how many times a model correctly classifies a negative sample as Negative?\n",
    "  \n",
    "     eg: Gender classifier model -> Female image => It is not a female\n",
    "     \n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    y_true = [2, 0, 2, 2, 0, 1]\n",
    "    y_pred = [0, 0, 2, 2, 0, 2]\n",
    "    confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    \n",
    "3. Precision\n",
    "    eg: Gender class --> the model classifies gender correctly \n",
    "    Precision = True Positive/(True Positive + False Positive)  \n",
    "                =TP / TP+FP\n",
    "              TP =2 , FP =1\n",
    "              =2/3 =0.66\n",
    "     1 -> 100% -> outstanding\n",
    "     0.75 -> Reasonable value but not outstanding\n",
    "     < 0.75 -> Improve   \n",
    "     \n",
    "     \n",
    "    eg:\n",
    "    from sklearn.metrics import precision_score\n",
    "    y_true = [0, 1, 2, 0, 1, 2]\n",
    "    y_pred = [0, 2, 1, 0, 0, 1]\n",
    "    precision_score(y_true, y_pred)\n",
    "\n",
    "4.Recall -> This model corectly identified the o/p\n",
    "      Recall = True Positive/(True Positive + False Negative)\n",
    "              =TP/TP+FN\n",
    "              \n",
    "       TP = 2\n",
    "       FN= 1  \n",
    "       Recall= 2/3=0.66\n",
    "       \n",
    "       0.75 -> good\n",
    "       <0.75 -> improve\n",
    "       1- outstanding \n",
    "       0 -> bad\n",
    "       \n",
    "    eg:\n",
    "       from sklearn.metrics import recall_score\n",
    "        y_true = [0, 1, 2, 0, 1, 2]\n",
    "        y_pred = [0, 2, 1, 0, 0, 1]\n",
    "        recall_score(y_true, y_pred)   \n",
    "        \n",
    "5.AUC score - Area Under the Receiver Operating Characteristic Curve (ROC AUC)\n",
    "\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    y_true = [0,1,1,0,0,1]\n",
    "    y_pred = [0,0,1,1,0,1]\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "    AUC score\tInterpretation\n",
    "    >0.8\tVery good performance\n",
    "    0.7-0.8\tGood performance\n",
    "    0.5-0.7\tOK performance\n",
    "    <0.5  As good as random choice\n",
    "    \n",
    "How can I improve my AUC score?\n",
    "To improve your AUC score there are three things that you could do:\n",
    "1.Add more features to your dataset which provide some signal for the target\n",
    "2.Tweak your model by adjusting parameters or the type of model used\n",
    "3.Change the probability threshold at which the classes are chosen\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
